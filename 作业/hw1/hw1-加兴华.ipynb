{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预备：申明模块\n",
    "\n",
    "使用前需要定义下述变量：\n",
    "\n",
    "gamma--衰减率\n",
    "\n",
    "gridsize--世界的行列数\n",
    "\n",
    "goal--目标坐标\n",
    "\n",
    "bad--惩罚区域坐标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# 创建坐标点类\n",
    "class Agrid:\n",
    "    def __init__(self,x=0,y=0):\n",
    "        self.x=x  # 第几行\n",
    "        self.y=y  # 第几列\n",
    "        self.acts=self.create_acts() # 可以采取的行动\n",
    "        self.rewards=self.create_rewards() # 上述行动对应的回报\n",
    "        self.v=0  # VI：初始化值函数为0\n",
    "        self.pi=0,0 # PI: 初始化策略为原地不动\n",
    "\n",
    "    def create_acts(self):\n",
    "        acts=[(-1,0),(0,-1),(1,0),(0,1),(0,0)]  # 对应↑ ← ↓ → o\n",
    "        # 根据约束去除\n",
    "        if self.x==0: acts.remove((-1,0))\n",
    "        if self.y==0: acts.remove((0,-1))\n",
    "        if self.x+1==gridsize[0]: acts.remove((1,0))\n",
    "        if self.y+1==gridsize[1]: acts.remove((0,1))\n",
    "        if self.x==goal[0] and self.y==goal[1]: acts=[(0,0)]\n",
    "        return acts\n",
    "    \n",
    "    def create_rewards(self):\n",
    "        answer=[]\n",
    "        for a in self.acts:\n",
    "            # 不动就没有回报\n",
    "            if a==(0,0): \n",
    "                answer.append(0) \n",
    "            # 遇到障碍\n",
    "            elif (self.x+a[0],self.y+a[1]) in bad: \n",
    "                answer.append(-1)\n",
    "            # 到达目标\n",
    "            elif (self.x+a[0],self.y+a[1])==goal: \n",
    "                answer.append(1)\n",
    "            # 其他情况\n",
    "            else: \n",
    "                answer.append(0)  \n",
    "        return answer\n",
    "\n",
    "# 值提升\n",
    "def VI(grid=[[Agrid(i,j) for j in range(gridsize[1])] for i in range(gridsize[0])]):\n",
    "    eps = 1e-4\n",
    "    while True:\n",
    "        last_grid=copy.deepcopy(grid)\n",
    "        error=0\n",
    "        for i in range(gridsize[0]):\n",
    "            for j in range(gridsize[1]):\n",
    "                value=last_grid[i][j].v\n",
    "                for a in grid[i][j].acts:\n",
    "                    index=grid[i][j].acts.index(a)\n",
    "                    # 值迭代公式\n",
    "                    now_v=grid[i][j].rewards[index]+ gamma*last_grid[i+a[0]][j+a[1]].v\n",
    "                    # print(now_v)\n",
    "                    if now_v>value:\n",
    "                        value=now_v\n",
    "                        grid[i][j].pi=a\n",
    "                grid[i][j].v=value\n",
    "                error+=value-last_grid[i][j].v\n",
    "        if error<eps: \n",
    "            return grid\n",
    "\n",
    "# 策略提升\n",
    "def PI(grid=[[Agrid(i,j) for j in range(gridsize[1])] for i in range(gridsize[0])]):\n",
    "    eps = 1e-4\n",
    "    while True:\n",
    "        last_grid_for_pi=copy.deepcopy(grid)\n",
    "        flag=False   # 判断策略提升一步前后有无策略变化\n",
    "        # 策略评估\n",
    "        while True:\n",
    "            last_grid=copy.deepcopy(grid)\n",
    "            error=0\n",
    "            for i in range(gridsize[0]):\n",
    "                for j in range(gridsize[1]):\n",
    "                    pi=grid[i][j].pi\n",
    "                    index=grid[i][j].acts.index(pi)\n",
    "                    # 迭代公式\n",
    "                    grid[i][j].v=grid[i][j].rewards[index]+\\\n",
    "                                    gamma*last_grid[i+pi[0]][j+pi[1]].v \n",
    "                    error+=abs(grid[i][j].v-last_grid[i][j].v)\n",
    "            if error<=eps: break\n",
    "\n",
    "        # 策略提升\n",
    "        for i in range(gridsize[0]):\n",
    "            for j in range(gridsize[1]):\n",
    "                value=grid[i][j].v\n",
    "                # 遍历所有可能动作取最大\n",
    "                for a in grid[i][j].acts:\n",
    "                    index=grid[i][j].acts.index(a)\n",
    "                    # 值迭代公式\n",
    "                    now_v=grid[i][j].rewards[index]+ gamma*last_grid[i+a[0]][j+a[1]].v\n",
    "                    if now_v>value:\n",
    "                        value=now_v\n",
    "                        grid[i][j].pi=a\n",
    "                # 判断策略是否改变\n",
    "                if last_grid_for_pi[i][j].pi!=grid[i][j].pi: flag=True\n",
    "        if flag==False: \n",
    "            return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\gamma$ =0.9 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.9\n",
    "gridsize=4,4     # 行，列\n",
    "goal=2,2\n",
    "bad=[(1,2),(2,1),(3,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "值迭代部分\n",
      "状态值函数：\n",
      "0.590 0.656 0.729 0.810 \n",
      "\n",
      "0.531 0.590 1.000 0.900 \n",
      "\n",
      "0.478 1.000 0.000 1.000 \n",
      "\n",
      "0.430 0.900 1.000 0.900 \n",
      "\n",
      "策略：\n",
      "→ → → ↓ \n",
      "\n",
      "↑ ↑ ↓ ↓ \n",
      "\n",
      "↑ → o ← \n",
      "\n",
      "↑ → ↑ ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('值迭代部分')\n",
    "print('状态值函数：')\n",
    "answer=VI()\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(\"{:.3f}\".format(answer[i][j].v),end=' ')\n",
    "    print('\\n')\n",
    "print('策略：')\n",
    "acts=[(-1,0),(0,-1),(1,0),(0,1),(0,0)]\n",
    "vis=['↑', '←', '↓', '→', 'o']\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(vis[acts.index(answer[i][j].pi)],end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略迭代部分\n",
      "状态值函数：\n",
      "0.590 0.656 0.729 0.810 \n",
      "\n",
      "0.531 0.590 1.000 0.900 \n",
      "\n",
      "0.478 1.000 0.000 1.000 \n",
      "\n",
      "0.430 0.900 1.000 0.900 \n",
      "\n",
      "策略：\n",
      "→ → → ↓ \n",
      "\n",
      "↑ ↑ ↓ ↓ \n",
      "\n",
      "↑ → o ← \n",
      "\n",
      "↑ → ↑ ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('策略迭代部分')\n",
    "print('状态值函数：')\n",
    "answer=PI()\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(\"{:.3f}\".format(answer[i][j].v),end=' ')\n",
    "    print('\\n')\n",
    "print('策略：')\n",
    "acts=[(-1,0),(0,-1),(1,0),(0,1),(0,0)]\n",
    "vis=['↑', '←', '↓', '→', 'o']\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(vis[acts.index(answer[i][j].pi)],end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\gamma$=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma=0.5\n",
    "gridsize=4,4     # 行，列\n",
    "goal=2,2\n",
    "bad=[(1,2),(2,1),(3,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "值迭代部分\n",
      "状态值函数：\n",
      "0.031 0.062 0.125 0.250 \n",
      "\n",
      "0.016 0.031 1.000 0.500 \n",
      "\n",
      "0.008 1.000 0.000 1.000 \n",
      "\n",
      "0.004 0.500 1.000 0.500 \n",
      "\n",
      "策略：\n",
      "→ → → ↓ \n",
      "\n",
      "↑ ↑ ↓ ↓ \n",
      "\n",
      "↑ → o ← \n",
      "\n",
      "↑ → ↑ ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('值迭代部分')\n",
    "print('状态值函数：')\n",
    "answer=VI()\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(\"{:.3f}\".format(answer[i][j].v),end=' ')\n",
    "    print('\\n')\n",
    "print('策略：')\n",
    "acts=[(-1,0),(0,-1),(1,0),(0,1),(0,0)]\n",
    "vis=['↑', '←', '↓', '→', 'o']\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(vis[acts.index(answer[i][j].pi)],end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "策略迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "策略迭代部分\n",
      "状态值函数：\n",
      "0.031 0.062 0.125 0.250 \n",
      "\n",
      "0.016 0.031 1.000 0.500 \n",
      "\n",
      "0.008 1.000 0.000 1.000 \n",
      "\n",
      "0.004 0.500 1.000 0.500 \n",
      "\n",
      "策略：\n",
      "→ → → ↓ \n",
      "\n",
      "↑ ↑ ↓ ↓ \n",
      "\n",
      "↑ → o ← \n",
      "\n",
      "↑ → ↑ ↑ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('策略迭代部分')\n",
    "print('状态值函数：')\n",
    "answer=PI()\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(\"{:.3f}\".format(answer[i][j].v),end=' ')\n",
    "    print('\\n')\n",
    "print('策略：')\n",
    "acts=[(-1,0),(0,-1),(1,0),(0,1),(0,0)]\n",
    "vis=['↑', '←', '↓', '→', 'o']\n",
    "for i in range(gridsize[0]):\n",
    "    for j in range(gridsize[1]):\n",
    "        print(vis[acts.index(answer[i][j].pi)],end=' ')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "观察发现，$\\gamma=0.9$ 和 $\\gamma=0.5$ 最终的最优策略是一样的，都是会绕过障碍。\n",
    "\n",
    "我认为这是因为本题中的模型比较简单，不碰到障碍的行为回报为0，也即无代价；而目标又只有一个，也即一条路径最多只有1个正回报。因此在提取策略时不经过障碍的路径收益一定高于经过障碍的，策略就表现为不采取经过障碍的行动。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6dc47ed52be77767ca04354fe6b66fe15b569d4eb17d13051d7217fb5a5c1b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
